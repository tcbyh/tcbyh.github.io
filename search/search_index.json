{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TCBYH","text":"<p>personal site by tcbyh</p>"},{"location":"#contents","title":"Contents","text":""},{"location":"#paper","title":"Paper","text":""},{"location":"#code","title":"Code","text":""},{"location":"#toolbox","title":"Toolbox","text":""},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/12/26/docker-%E6%89%8B%E5%86%8C/","title":"Docker \u624b\u518c","text":""},{"location":"blog/2024/12/26/git-%E6%89%8B%E5%86%8C/","title":"Git \u624b\u518c","text":""},{"location":"blog/2024/12/26/ubuntu-2204-lts-%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/","title":"Ubuntu 22.04 LTS \u7cfb\u7edf\u5b89\u88c5","text":""},{"location":"blog/2024/12/26/ubuntu-2204-lts-%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/#u","title":"\u542f\u52a8U\u76d8\u5236\u4f5c &amp; \u7cfb\u7edf\u5b89\u88c5","text":"<ul> <li>Ubuntu \u4e0b\u8f7d\uff1ahttps://ubuntu.com/download/desktop</li> <li>Ventoy \u4e0b\u8f7d &amp; \u5236\u4f5c\uff1ahttps://www.ventoy.net/cn/download.html</li> <li>Ubuntu \u5b89\u88c5\u6307\u5357\uff1ahttps://ubuntu.com/tutorials/install-ubuntu-desktop</li> </ul>"},{"location":"blog/2024/12/26/ubuntu-2204-lts-%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/#_1","title":"\u95ee\u9898:","text":"<ul> <li>\u542f\u52a8\u9ed1\u5c4f/\u6a2a\u7ebf ===&gt; \u9009 Safe (Graphics) \u8fdb\u5165 https://blog.csdn.net/weixin_44478317/article/details/142670119</li> </ul>"},{"location":"blog/2024/12/26/ubuntu-2204-lts-%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/#_2","title":"\u8f6f\u4ef6\u6e90\u914d\u7f6e","text":""},{"location":"blog/2024/12/26/ubuntu-2204-lts-%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/#apt","title":"APT\u6e90","text":"<p>https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/</p> <p>\u66ff\u6362 <code>/etc/apt/sources.list</code></p> <pre><code># \u9ed8\u8ba4\u6ce8\u91ca\u4e86\u6e90\u7801\u955c\u50cf\u4ee5\u63d0\u9ad8 apt update \u901f\u5ea6\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u53d6\u6d88\u6ce8\u91ca\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse\n# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse\n\n# \u4ee5\u4e0b\u5b89\u5168\u66f4\u65b0\u8f6f\u4ef6\u6e90\u5305\u542b\u4e86\u5b98\u65b9\u6e90\u4e0e\u955c\u50cf\u7ad9\u914d\u7f6e\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u4fee\u6539\u6ce8\u91ca\u5207\u6362\ndeb http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse\n# deb-src http://security.ubuntu.com/ubuntu/ jammy-security main restricted universe multiverse\n\n# \u9884\u53d1\u5e03\u8f6f\u4ef6\u6e90\uff0c\u4e0d\u5efa\u8bae\u542f\u7528\n# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse\n# # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse\n</code></pre> <p>\u7cfb\u7edf\u66f4\u65b0</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre>"},{"location":"blog/2024/12/26/ubuntu-2204-lts-%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/#conda","title":"Conda\u6e90","text":"<p>https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/</p> <p>\u5b89\u88c5 conda</p> <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nsh Miniconda3-latest-Linux-x86_64.sh\n</code></pre> <p>\u66ff\u6362 <code>~/.condarc</code></p> <pre><code>channels:\n  - defaults\nshow_channel_urls: true\ndefault_channels:\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2\ncustom_channels:\n  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n</code></pre>"},{"location":"blog/2024/12/26/ubuntu-2204-lts-%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/#pypi","title":"PyPI\u6e90\uff1a","text":"<p>\u547d\u4ee4\u884c\u8bbe\u7f6e</p> <pre><code>pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple\n</code></pre>"},{"location":"blog/2024/12/26/ubuntu-2204-lts-%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/#cuda-cudnn","title":"\u663e\u5361\u9a71\u52a8 &amp; Cuda &amp; CuDNN \u5b89\u88c5","text":"<p>https://gist.github.com/huxycn/a76cc4a99c6e06fece47cfef1f70f623</p>"},{"location":"blog/2024/12/26/ubuntu-2204-lts-%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/#_3","title":"\u5e38\u7528\u8f6f\u4ef6\u5b89\u88c5","text":"<ul> <li>apt install</li> <li>tree / vim / git</li> <li>terminator</li> <li> <p>mpv\uff1ahttps://fruit.je/apt</p> </li> <li> <p>deb \u76f4\u63a5\u5b89\u88c5</p> </li> <li>google-chrome</li> <li>sougoupinyin<ul> <li>issue1: \u5b89\u88c5\u5931\u8d25 ==&gt; \u4e25\u683c\u6309\u7167\u6b65\u9aa4\u5b89\u88c5: https://shurufa.sogou.com/linux/guide</li> <li>issue2: \u8bef\u5207\u6362\u7e41\u4f53\u6a21\u5f0f ===&gt; Ctrl + Shift + F \u5207\u6362\u56de\u7b80\u4f53\u6a21\u5f0f</li> <li>issue3: Chrome\u4e2d\u65e0\u6cd5\u4f7f\u7528\u8f93\u5165\u6cd5 ===&gt; sudo apt install fcitx5-frontend-gtk4 https://zhuanlan.zhihu.com/p/1895373648698787278</li> </ul> </li> <li>code</li> <li>feishu</li> <li>easyconnect</li> <li>cloudpc</li> </ul>"},{"location":"blog/2024/12/26/%E9%98%85%E8%AF%BB%E6%95%B4%E7%90%86%E6%96%87%E7%8C%AE%E6%96%B9%E6%B3%95/","title":"\u9605\u8bfb\u6574\u7406\u6587\u732e\u65b9\u6cd5","text":"<p>\u5982\u4f55\u5feb\u901f\u9605\u8bfb\u548c\u6574\u7406\u6587\u732e\uff0c\u5982\u4f55\u7cbe\u5ea6\u8bba\u6587 -- MLi</p>"},{"location":"blog/2024/12/26/%E9%98%85%E8%AF%BB%E6%95%B4%E7%90%86%E6%96%87%E7%8C%AE%E6%96%B9%E6%B3%95/#1","title":"1. \u5982\u4f55\u5feb\u901f\u9605\u8bfb\u548c\u6574\u7406\u6587\u732e","text":"<p>https://www.bilibili.com/video/BV1nA41157y4</p> <p>\u5229\u7528 markdown \u6536\u96c6\u6574\u7406\u6587\u732e - \u6bcf\u7bc7\u8bba\u6587\u6dfb\u52a0\u7b80\u5355\u7b14\u8bb0\uff0c\u91cd\u8981\u56fe\u8868\uff0c\u91cd\u8981\u516c\u5f0f - \u591a\u7bc7\u8bba\u6587\u76f8\u4e92\u5173\u8054\uff0c\u7b80\u8981\u6807\u8bb0\u6539\u8fdb\u70b9</p> <p> </p> <p>\u5de5\u5177\uff1atypora\uff0cmarkdown\uff0cmermaid\uff0carxiv_tool</p>"},{"location":"blog/2024/12/26/%E9%98%85%E8%AF%BB%E6%95%B4%E7%90%86%E6%96%87%E7%8C%AE%E6%96%B9%E6%B3%95/#2","title":"2. \u5982\u4f55\u7cbe\u8bfb\u8bba\u6587","text":"<p>https://www.bilibili.com/video/BV1H44y1t75x</p> <ol> <li>title (author)</li> <li>abstract</li> <li>introduction</li> <li>method: figures / tables</li> <li>experiments: figures / tables</li> <li> <p>conclusion</p> </li> <li> <p>\u7b2c\u4e00\u904d\uff1a\u6807\u9898\u3001\u6458\u8981\u3001\u7ed3\u8bba\u3002\u53ef\u4ee5\u770b\u4e00\u770b\u65b9\u6cd5\u548c\u5b9e\u9a8c\u90e8\u5206\u91cd\u8981\u7684\u56fe\u548c\u8868\u3002\u8fd9\u6837\u53ef\u4ee5\u82b1\u8d39\u5341\u51e0\u5206\u949f\u65f6\u95f4\u4e86\u89e3\u5230\u8bba\u6587\u662f\u5426\u9002\u5408\u4f60\u7684\u7814\u7a76\u65b9\u5411\u3002</p> </li> <li>\u7b2c\u4e8c\u904d\uff1a\u786e\u5b9a\u8bba\u6587\u503c\u5f97\u8bfb\u4e4b\u540e\uff0c\u53ef\u4ee5\u5feb\u901f\u7684\u628a\u6574\u4e2a\u8bba\u6587\u8fc7\u4e00\u904d\uff0c\u4e0d\u9700\u8981\u77e5\u9053\u6240\u6709\u7684\u7ec6\u8282\uff0c\u9700\u8981\u4e86\u89e3\u91cd\u8981\u7684\u56fe\u548c\u8868\uff0c\u77e5\u9053\u6bcf\u4e00\u4e2a\u90e8\u5206\u5728\u5e72\u4ec0\u4e48\uff0c\u5708\u51fa\u76f8\u5173\u6587\u732e\u3002\u89c9\u5f97\u6587\u7ae0\u592a\u96be\uff0c\u53ef\u4ee5\u8bfb\u5f15\u7528\u7684\u6587\u732e\u3002</li> <li>\u7b2c\u4e09\u904d\uff1a\u63d0\u51fa\u4ec0\u4e48\u95ee\u9898\uff0c\u7528\u4ec0\u4e48\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u5b9e\u9a8c\u662f\u600e\u4e48\u505a\u7684\u3002\u5408\u4e0a\u6587\u7ae0\uff0c\u56de\u5fc6\u6bcf\u4e00\u4e2a\u90e8\u5206\u5728\u8bb2\u4ec0\u4e48\u3002</li> </ol>"},{"location":"blog/2024/12/26/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA3d-reconstruction/","title":"\u4e09\u7ef4\u91cd\u5efa\uff083D Reconstruction\uff09","text":"<p>\u4e09\u7ef4\u91cd\u5efa\u76f8\u5173\u8bba\u6587\uff0cNeRF\u30013DGS \u7b49\u7b49 ...</p>"},{"location":"blog/2024/12/26/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA3d-reconstruction/#_1","title":"\u57fa\u7840\u6a21\u578b","text":"<ul> <li> <p>NeRF: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. Ben Mildenhall et al. 2020. arxiv pdf code articles[1] videos[1]</p> </li> <li> <p>3DGS: 3D Gaussian Splatting for Real-Time Radiance Field Rendering. Bernhard Kerbl et al. 2023. arxiv pdf code videos[1]</p> </li> </ul>"},{"location":"blog/2024/12/26/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA3d-reconstruction/#road-surface-reconstruction","title":"\u8def\u9762\u91cd\u5efa\uff08Road Surface Reconstruction\uff09","text":"<ul> <li> <p>RoMe: RoMe: Towards Large Scale Road Surface Reconstruction via Mesh  Representation. Ruohong Mei et al. 2023. arxiv pdf code</p> </li> <li> <p>RoGs: RoGs: Large Scale Road Surface Reconstruction with Meshgrid Gaussian. Zhiheng Feng et al. 2024. arxiv pdf code</p> </li> </ul>"},{"location":"blog/2024/12/26/detr-%E6%A1%86%E6%9E%B6%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6/","title":"DETR \u6846\u67b6\u76f8\u5173\u7814\u7a76","text":"<p>DETR \u6846\u67b6\u57fa\u672c\u67b6\u6784\u4e0e\u539f\u7406\uff0c\u540e\u7eed\u6539\u8fdb\uff0c\u4ee5\u53ca\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\uff0c\u5305\u62ec\u68c0\u6d4b\u3001\u5206\u5272\u7b49\u7b49 ...</p> <p>Reference: 1. detrex: https://github.com/IDEA-Research/detrex</p> <ul> <li> <p>DETR: End-to-End Object Detection with Transformers. Nicolas Carion et al. 2020. arxiv pdf </p> </li> <li> <p>Deformable-DETR: Deformable DETR: Deformable Transformers for End-to-End Object Detection. Xizhou Zhu et al. 2020. arxiv pdf</p> </li> <li>Conditional-DETR: Conditional DETR for Fast Training Convergence. Depu Meng et al. 2021. arxiv pdf</li> <li>Anchor-DETR: Anchor DETR: Query Design for Transformer-Based Object Detection. Yingming Wang et al. 2021. arxiv pdf</li> <li>DAB-DETR: DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR. Shilong Liu et al. 2022. arxiv pdf</li> <li>DN-DETR: DN-DETR: Accelerate DETR Training by Introducing Query DeNoising. Feng Li et al. 2022. arxiv pdf</li> <li>DINO: DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object  Detection. Hao Zhang et al. 2022. arxiv pdf</li> <li>Co-DETR: DETRs with Collaborative Hybrid Assignments Training. Zhuofan Zong et al. 2022. arxiv pdf</li> </ul> <pre><code>graph TD\nDETR --MS Deform Attn--&gt; Deformable-DETR\nDETR --&gt; Conditional-DETR\n</code></pre>"},{"location":"blog/2024/12/26/detr-%E6%A1%86%E6%9E%B6%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6/#sam-series","title":"SAM series","text":"<ul> <li>SAM: Segment Anything. Alexander Kirillov et al. 2023. arxiv pdf</li> <li>SAM2: SAM 2: Segment Anything in Images and Videos. Nikhila Ravi et al. 2024. arxiv pdf</li> </ul>"},{"location":"blog/2024/12/26/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/","title":"\u6269\u6563\u6a21\u578b","text":"<p>\u6269\u6563\u6a21\u578b</p> <ul> <li>{{https://arxiv.org/abs/2303.09769}}</li> </ul> <ol> <li>\u6700\u65b0\u5de5\u4f5c</li> <li> <p>HeKaiming: {{https://arxiv.org/abs/2406.11838}}</p> </li> <li> <p>https://huggingface.co/blog/annotated-diffusion</p> </li> </ol> <ul> <li> <p>GAN: Generative Adversarial Nets. Ian J. Goodfellow et al. 2014. arxiv pdf</p> </li> <li> <p>DDPMs: Denoising Diffusion Probabilistic Models. Jonathan Ho et al. 2020. arxiv pdf</p> </li> <li> <p>DiT: Scalable Diffusion Models with Transformers. William Peebles et al. 2022. arxiv pdf</p> </li> <li> <p>SiT: SiT: Exploring Flow and Diffusion-based Generative Models with Scalable  Interpolant Transformers. Nanye Ma et al. 2024. arxiv pdf</p> </li> </ul> <ol> <li>\u7ecf\u5178\u8bba\u6587\u63a8\u8350\uff1ahttps://zhuanlan.zhihu.com/p/595866176</li> </ol> <ul> <li>DDPM: Denoising Diffusion Probabilistic Models. Jonathan Ho et al. 2020. arxiv pdf</li> <li>\u89e3\u8bfb</li> <li>Machine Learning 2023 Spring - Hung-yi Lee (\u674e\u5b8f\u6bc5) #Diffusion Model \u539f\u7406\u5256\u6790</li> <li>UnderstandingDiffusionModels_AUnifiedPerspective: {{https://arxiv.org/abs/2208.11970}}</li> <li>\u751f\u6210\u6269\u6563\u6a21\u578b\u6f2b\u8c08\uff08\u4e00\uff09\uff1aDDPM = \u62c6\u697c + \u5efa\u697c - \u79d1\u5b66\u7a7a\u95f4|Scientific Spaces </li> <li>\u4e00\u6587\u89e3\u91ca Diffusion Model (\u4e00) DDPM \u7406\u8bba\u63a8\u5bfc - \u6492\u65e6-cc\u7684\u6587\u7ae0 - \u77e5\u4e4e</li> <li>\u5b9e\u8df5</li> <li>\u5b9e\u73b0diffusion\u6a21\u578b(\u624b\u5199\u6570\u5b57\u96c6) blog code</li> <li>https://github.com/bot66/MNISTDiffusion</li> <li> <p>https://github.com/lucidrains/denoising-diffusion-pytorch</p> </li> <li> <p>DDIM: Denoising Diffusion Implicit Models. Jiaming Song et al. 2020. arxiv pdf</p> </li> <li>OpenAI: Diffusion Models Beat GANs on Image Synthesis. Prafulla Dhariwal et al. 2021. arxiv pdf</li> <li>ClassifierFreeDG: Classifier-Free Diffusion Guidance. Jonathan Ho et al. 2022. arxiv pdf</li> <li>GLIDE: GLIDE: Towards Photorealistic Image Generation and Editing with  Text-Guided Diffusion Models. Alex Nichol et al. 2021. arxiv pdf</li> <li>StableDiffusion: High-Resolution Image Synthesis with Latent Diffusion Models. Robin Rombach et al. 2021. arxiv pdf</li> <li>DiT: Scalable Diffusion Models with Transformers. William Peebles et al. 2022. arxiv pdf</li> <li>ControlNet: Adding Conditional Control to Text-to-Image Diffusion Models. Lvmin Zhang et al. 2023. arxiv pdf</li> <li>LoRA: LoRA: Low-Rank Adaptation of Large Language Models. Edward J. Hu et al. 2021. arxiv pdf</li> </ul>"},{"location":"blog/2024/12/26/reading-papers/","title":"Reading Papers","text":"<p>\u5f85\u6574\u7406 ...</p>"},{"location":"blog/2024/12/26/reading-papers/#pioneering-work","title":"Pioneering Work","text":"<p>Reference: 1.  2. DETR: https://b23.tv/Qy47ExG 3.  4. </p>"},{"location":"blog/2024/12/26/reading-papers/#downstream-task","title":"Downstream Task","text":""},{"location":"blog/2024/12/26/reading-papers/#backbone","title":"Backbone","text":""},{"location":"blog/2024/12/26/reading-papers/#object-detection","title":"Object Detection","text":""},{"location":"blog/2024/12/26/reading-papers/#image-segmentation","title":"Image Segmentation","text":"<p>Reference: 1. Universal Image Segmentation: https://huggingface.co/blog/mask2former</p> <ul> <li>MaskFormer: Per-Pixel Classification is Not All You Need for Semantic Segmentation. Bowen Cheng et al. 2021. arxiv pdf</li> <li>Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation. Bowen Cheng et al. 2021. arxiv pdf</li> <li>MaskDINO: Mask DINO: Towards A Unified Transformer-based Framework for Object  Detection and Segmentation. Feng Li et al. 2022. arxiv pdf</li> <li>Oneformer: OneFormer: One Transformer to Rule Universal Image Segmentation. Jitesh Jain et al. 2022. arxiv pdf</li> </ul>"},{"location":"blog/2024/12/26/reading-papers/#keypoint-detection","title":"Keypoint  Detection","text":"<ul> <li>ViTDet: Exploring Plain Vision Transformer Backbones for Object Detection. Yanghao Li et al. 2022. arxiv pdf</li> <li>ViTPose: ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation. Yufei Xu et al. 2022. arxiv pdf</li> <li>ViTPose++: ViTPose++: Vision Transformer for Generic Body Pose Estimation. Yufei Xu et al. 2022. arxiv pdf</li> </ul>"},{"location":"blog/2024/12/26/reading-papers/#learning-paradigm","title":"Learning Paradigm","text":""},{"location":"blog/2024/12/26/reading-papers/#contrastive-learning","title":"Contrastive Learning","text":"<p>Reference: 1. MoCo: https://b23.tv/khDz0mx 2. MoCo-v2/v3: https://b23.tv/hgy75AS</p> <ul> <li>MoCo: Momentum Contrast for Unsupervised Visual Representation Learning. Kaiming He et al. 2019. arxiv pdf</li> <li>MoCo-v2: Improved Baselines with Momentum Contrastive Learning. Xinlei Chen et al. 2020. arxiv pdf</li> <li>MoCo-v3: An Empirical Study of Training Self-Supervised Vision Transformers. Xinlei Chen et al. 2021. arxiv pdf</li> <li>DINO: Emerging Properties in Self-Supervised Vision Transformers. Mathilde Caron et al. 2021. arxiv pdf</li> <li>DINOv2: DINOv2: Learning Robust Visual Features without Supervision. Maxime Oquab et al. 2023. arxiv pdf</li> </ul>"},{"location":"blog/2024/12/26/reading-papers/#mask-image-modeling","title":"Mask Image Modeling","text":"<p>Reference: 1. MAE: https://b23.tv/RGS2Pu5</p> <ul> <li>MAE: Masked Autoencoders Are Scalable Vision Learners. Kaiming He et al. 2021. arxiv pdf</li> <li>BEiT: BEiT: BERT Pre-Training of Image Transformers. Hangbo Bao et al. 2021. arxiv pdf</li> <li>BEiT-v2: BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers. Zhiliang Peng et al. 2022. arxiv pdf</li> <li>BEiT-v3: Image as a Foreign Language: BEiT Pretraining for All Vision and  Vision-Language Tasks. Wenhui Wang et al. 2022. arxiv pdf</li> <li>EVA-01: EVA: Exploring the Limits of Masked Visual Representation Learning at  Scale. Yuxin Fang et al. 2022. arxiv pdf</li> <li>EVA-02: EVA-02: A Visual Representation for Neon Genesis. Yuxin Fang et al. 2023. arxiv pdf</li> </ul>"},{"location":"blog/2024/12/26/reading-papers/#active-learning","title":"Active Learning","text":"<ul> <li>SALOD: Scalable Active Learning for Object Detection. Elmar Haussmann et al. 2020. arxiv pdf</li> </ul>"},{"location":"blog/2024/12/26/reading-papers/#large-model","title":"Large Model","text":""},{"location":"blog/2024/12/26/reading-papers/#diffusion-model","title":"Diffusion Model","text":"<ul> <li>Stable-Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models. Robin Rombach et al. 2021. arxiv pdf</li> </ul>"},{"location":"blog/2024/12/26/reading-papers/#peft","title":"PEFT","text":"<p>Reference:  1. https://b23.tv/i9GVaAs 2. https://huggingface.co/blog/peft</p> <ul> <li> <p>LoRA: LoRA: Low-Rank Adaptation of Large Language Models. Edward J. Hu et al. 2021. arxiv pdf</p> <ul> <li>code: https://github.com/microsoft/LoRA</li> <li>Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.</li> <li>compared to adapters, no additional inference latency</li> </ul> <p></p> <p></p> </li> <li> <p>ViT-Adapter: Vision Transformer Adapter for Dense Predictions. Zhe Chen et al. 2022. arxiv pdf</p> </li> </ul>"},{"location":"blog/2024/12/26/reading-papers/#multi-modal","title":"Multi-Modal","text":"<p>open-vocabulary object detection:</p> <ul> <li>ViLD: Open-vocabulary Object Detection via Vision and Language Knowledge  Distillation. Xiuye Gu et al. 2021. arxiv pdf</li> <li>GroundingDINO: Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set  Object Detection. Shilong Liu et al. 2023. arxiv pdf</li> </ul> <p>class-agnostic object detection:</p> <ul> <li>MViTs: Class-agnostic Object Detection with Multi-modal Transformer. Muhammad Maaz et al. 2021. arxiv pdf</li> </ul>"},{"location":"blog/2024/12/26/reading-papers/#autonomous-driving","title":"Autonomous Driving","text":""},{"location":"blog/2024/12/26/reading-papers/#bev","title":"BEV","text":"<ul> <li>LSS: Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by  Implicitly Unprojecting to 3D. Jonah Philion et al. 2020. arxiv pdf</li> <li>BevDet: BEVDet: High-performance Multi-camera 3D Object Detection in  Bird-Eye-View. Junjie Huang et al. 2021. arxiv pdf</li> <li>BevVerse: BEVerse: Unified Perception and Prediction in Birds-Eye-View for  Vision-Centric Autonomous Driving. Yunpeng Zhang et al. 2022. arxiv pdf</li> <li>BevFormer: BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera  Images via Spatiotemporal Transformers. Zhiqi Li et al. 2022. arxiv pdf</li> </ul>"},{"location":"blog/2024/12/26/reading-papers/#occ","title":"Occ","text":"<ul> <li>NeRF: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. Ben Mildenhall et al. 2020. arxiv pdf</li> <li>3DGS: {{https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/}}</li> <li>TPVFormer: Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction. Yuanhui Huang et al. 2023. arxiv pdf</li> </ul>"},{"location":"blog/2024/12/26/reading-papers/#end-to-end","title":"End-to-End","text":"<ul> <li>UniAD: Planning-oriented Autonomous Driving. Yihan Hu et al. 2022. arxiv pdf</li> <li>E2EADSurvey: End-to-end Autonomous Driving: Challenges and Frontiers. Li Chen et al. 2023. arxiv pdf</li> </ul>"},{"location":"blog/2024/12/26/reading-papers/#vision-language-model","title":"Vision Language Model","text":""},{"location":"blog/2024/12/26/reading-papers/#world-model","title":"World Model","text":""},{"location":"blog/2024/12/26/%E9%98%85%E8%AF%BB%E6%95%B4%E7%90%86%E6%96%87%E7%8C%AE%E6%96%B9%E6%B3%95/","title":"\u9605\u8bfb\u6574\u7406\u6587\u732e\u65b9\u6cd5","text":"<p>\u5982\u4f55\u5feb\u901f\u9605\u8bfb\u548c\u6574\u7406\u6587\u732e\uff0c\u5982\u4f55\u7cbe\u5ea6\u8bba\u6587 -- MLi</p>"},{"location":"blog/2024/12/26/%E9%98%85%E8%AF%BB%E6%95%B4%E7%90%86%E6%96%87%E7%8C%AE%E6%96%B9%E6%B3%95/#1","title":"1. \u5982\u4f55\u5feb\u901f\u9605\u8bfb\u548c\u6574\u7406\u6587\u732e","text":"<p>https://www.bilibili.com/video/BV1nA41157y4</p> <p>\u5229\u7528 markdown \u6536\u96c6\u6574\u7406\u6587\u732e - \u6bcf\u7bc7\u8bba\u6587\u6dfb\u52a0\u7b80\u5355\u7b14\u8bb0\uff0c\u91cd\u8981\u56fe\u8868\uff0c\u91cd\u8981\u516c\u5f0f - \u591a\u7bc7\u8bba\u6587\u76f8\u4e92\u5173\u8054\uff0c\u7b80\u8981\u6807\u8bb0\u6539\u8fdb\u70b9</p> <p> </p> <p>\u5de5\u5177\uff1atypora\uff0cmarkdown\uff0cmermaid\uff0carxiv_tool</p>"},{"location":"blog/2024/12/26/%E9%98%85%E8%AF%BB%E6%95%B4%E7%90%86%E6%96%87%E7%8C%AE%E6%96%B9%E6%B3%95/#2","title":"2. \u5982\u4f55\u7cbe\u8bfb\u8bba\u6587","text":"<p>https://www.bilibili.com/video/BV1H44y1t75x</p> <ol> <li>title (author)</li> <li>abstract</li> <li>introduction</li> <li>method: figures / tables</li> <li>experiments: figures / tables</li> <li> <p>conclusion</p> </li> <li> <p>\u7b2c\u4e00\u904d\uff1a\u6807\u9898\u3001\u6458\u8981\u3001\u7ed3\u8bba\u3002\u53ef\u4ee5\u770b\u4e00\u770b\u65b9\u6cd5\u548c\u5b9e\u9a8c\u90e8\u5206\u91cd\u8981\u7684\u56fe\u548c\u8868\u3002\u8fd9\u6837\u53ef\u4ee5\u82b1\u8d39\u5341\u51e0\u5206\u949f\u65f6\u95f4\u4e86\u89e3\u5230\u8bba\u6587\u662f\u5426\u9002\u5408\u4f60\u7684\u7814\u7a76\u65b9\u5411\u3002</p> </li> <li>\u7b2c\u4e8c\u904d\uff1a\u786e\u5b9a\u8bba\u6587\u503c\u5f97\u8bfb\u4e4b\u540e\uff0c\u53ef\u4ee5\u5feb\u901f\u7684\u628a\u6574\u4e2a\u8bba\u6587\u8fc7\u4e00\u904d\uff0c\u4e0d\u9700\u8981\u77e5\u9053\u6240\u6709\u7684\u7ec6\u8282\uff0c\u9700\u8981\u4e86\u89e3\u91cd\u8981\u7684\u56fe\u548c\u8868\uff0c\u77e5\u9053\u6bcf\u4e00\u4e2a\u90e8\u5206\u5728\u5e72\u4ec0\u4e48\uff0c\u5708\u51fa\u76f8\u5173\u6587\u732e\u3002\u89c9\u5f97\u6587\u7ae0\u592a\u96be\uff0c\u53ef\u4ee5\u8bfb\u5f15\u7528\u7684\u6587\u732e\u3002</li> <li>\u7b2c\u4e09\u904d\uff1a\u63d0\u51fa\u4ec0\u4e48\u95ee\u9898\uff0c\u7528\u4ec0\u4e48\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u5b9e\u9a8c\u662f\u600e\u4e48\u505a\u7684\u3002\u5408\u4e0a\u6587\u7ae0\uff0c\u56de\u5fc6\u6bcf\u4e00\u4e2a\u90e8\u5206\u5728\u8bb2\u4ec0\u4e48\u3002</li> </ol>"},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/","title":"\u8bba\u6587\u6c47\u603b","text":"<p>\u8bba\u6587\u6c47\u603b ...</p>"},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#_2","title":"\u5f00\u521b\u6027\u5de5\u4f5c","text":"<ul> <li>Transformer: 1706.03762. Attention Is All You Need. Ashish Vaswani et al | [abs] [pdf]</li> <li>ViT: 2010.11929. An Image is Worth 16x16 Words: Transformers for Image Recognition at  Scale. Alexey Dosovitskiy et al | [abs] [pdf]</li> <li>Swin Transformer: 2103.14030. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. Ze Liu et al | [abs] [pdf]</li> <li>ViT Det: 2203.16527. Exploring Plain Vision Transformer Backbones for Object Detection. Yanghao Li et al | [abs] [pdf]</li> <li>DETR: 2005.12872. End-to-End Object Detection with Transformers. Nicolas Carion et al | [abs] [pdf]</li> <li>CLIP: 2103.00020. Learning Transferable Visual Models From Natural Language Supervision. Alec Radford et al | [abs] [pdf]</li> </ul>"},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#_3","title":"\u4e0b\u6e38\u4efb\u52a1","text":""},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#_4","title":"\u76ee\u6807\u68c0\u6d4b","text":"<ul> <li>Deformable DETR: 2010.04159. Deformable DETR: Deformable Transformers for End-to-End Object Detection. Xizhou Zhu et al | [abs] [pdf]</li> <li>DAB-DETR: 2201.12329. DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR. Shilong Liu et al | [abs] [pdf]</li> <li>DN-DETR: 2203.01305. DN-DETR: Accelerate DETR Training by Introducing Query DeNoising. Feng Li et al | [abs] [pdf]</li> <li>DINO: 2203.03605. DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object  Detection. Hao Zhang et al | [abs] [pdf]</li> <li>Co-DETR: 2211.12860. DETRs with Collaborative Hybrid Assignments Training. Zhuofan Zong et al | [abs] [pdf]</li> </ul>"},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#_5","title":"\u56fe\u50cf\u5206\u5272","text":"<ul> <li>MaskFormer: 2107.06278. Per-Pixel Classification is Not All You Need for Semantic Segmentation. Bowen Cheng et al | [abs] [pdf]</li> <li>Mask2Former: 2112.01527. Masked-attention Mask Transformer for Universal Image Segmentation. Bowen Cheng et al | [abs] [pdf]</li> <li>MaskDINO: 2206.02777. Mask DINO: Towards A Unified Transformer-based Framework for Object  Detection and Segmentation. Feng Li et al | [abs] [pdf]</li> <li>Oneformer: 2211.06220. OneFormer: One Transformer to Rule Universal Image Segmentation. Jitesh Jain et al | [abs] [pdf]</li> </ul>"},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#_6","title":"\u5173\u952e\u70b9\u68c0\u6d4b","text":"<ul> <li>ViTPose: 2204.12484. ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation. Yufei Xu et al | [abs] [pdf]</li> <li>ViTPose++: 2212.04246. ViTPose++: Vision Transformer Foundation Model for Generic Body Pose  Estimation. Yufei Xu et al | [abs] [pdf]</li> </ul>"},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#_7","title":"\u5b66\u4e60\u8303\u5f0f","text":""},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#_8","title":"\u81ea\u76d1\u7763\u5b66\u4e60","text":""},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#_9","title":"\u5bf9\u6bd4\u5b66\u4e60","text":"<ul> <li>MoCo: 1911.05722. Momentum Contrast for Unsupervised Visual Representation Learning. Kaiming He et al | [abs] [pdf]</li> <li>MoCo v2: 2003.04297. Improved Baselines with Momentum Contrastive Learning. Xinlei Chen et al | [abs] [pdf]</li> <li>2104.02057. An Empirical Study of Training Self-Supervised Vision Transformers. Xinlei Chen et al | [abs] [pdf]</li> <li>2104.14294. Emerging Properties in Self-Supervised Vision Transformers. Mathilde Caron et al | [abs] [pdf]</li> <li>DINOv2: 2304.07193. DINOv2: Learning Robust Visual Features without Supervision. Maxime Oquab et al | [abs] [pdf]</li> </ul>"},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#_10","title":"\u63a9\u7801\u5b66\u4e60","text":"<ul> <li>MAE: 2111.06377. Masked Autoencoders Are Scalable Vision Learners. Kaiming He et al | [abs] [pdf]</li> <li>BEiT: 2106.08254. BEiT: BERT Pre-Training of Image Transformers. Hangbo Bao et al | [abs] [pdf]</li> <li>BEiT v2: 2208.06366. BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers. Zhiliang Peng et al | [abs] [pdf]</li> <li>BEiT v3: 2208.10442. Image as a Foreign Language: BEiT Pretraining for All Vision and  Vision-Language Tasks. Wenhui Wang et al | [abs] [pdf]</li> <li>EVA-01: 2211.07636. EVA: Exploring the Limits of Masked Visual Representation Learning at  Scale. Yuxin Fang et al | [abs] [pdf]</li> <li>EVA-02: 2303.11331. EVA-02: A Visual Representation for Neon Genesis. Yuxin Fang et al | [abs] [pdf]</li> </ul>"},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#_11","title":"\u4e3b\u52a8\u5b66\u4e60","text":"<ul> <li>SALOD: 2004.04699. Scalable Active Learning for Object Detection. Elmar Haussmann et al | [abs] [pdf]</li> </ul>"},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#_12","title":"\u5927\u6a21\u578b","text":""},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#_13","title":"\u6269\u6563\u6a21\u578b","text":"<p>Diffusion Model: 2112.10752. High-Resolution Image Synthesis with Latent Diffusion Models. Robin Rombach et al | [abs] [pdf]</p>"},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#peft","title":"PEFT","text":"<ul> <li>LoRA: 2106.09685. LoRA: Low-Rank Adaptation of Large Language Models. Edward J. Hu et al | [abs] [pdf]</li> <li>ViT-Adapter: 2205.08534. Vision Transformer Adapter for Dense Predictions. Zhe Chen et al | [abs] [pdf]</li> <li>2303.10512. Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning. Qingru Zhang et al | [abs] [pdf]</li> <li>QLoRA: 2305.14314. QLoRA: Efficient Finetuning of Quantized LLMs. Tim Dettmers et al | [abs] [pdf]</li> </ul>"},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#_14","title":"\u81ea\u52a8\u9a7e\u9a76","text":""},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#bev","title":"BEV","text":"<ul> <li>LSS: {{https://arxiv.org/abs/2008.05711}}</li> <li>BevDet: {{https://arxiv.org/abs/2112.11790}}</li> <li>BevVerse: {{https://arxiv.org/abs/2205.09743}}</li> <li>BevFormer: {{https://arxiv.org/abs/2203.17270}}</li> </ul>"},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#_15","title":"\u5360\u636e\u7f51\u7edc","text":""},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#_16","title":"\u7aef\u5230\u7aef","text":"<ul> <li>UniAD: 2212.10156. Planning-oriented Autonomous Driving. Yihan Hu et al | [abs] [pdf]</li> <li>Survey: 2306.16927. End-to-end Autonomous Driving: Challenges and Frontiers. Li Chen et al | [abs] [pdf]</li> </ul>"},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#llmvision","title":"\u573a\u666f\u7406\u89e3 (LLM+Vision)","text":""},{"location":"blog/2024/12/26/%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB/#predict-next-frame","title":"\u4e16\u754c\u6a21\u578b (predict next frame)","text":""},{"location":"blog/2024/12/26/transformer-%E6%9E%B6%E6%9E%84%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6/","title":"Transformer \u67b6\u6784\u76f8\u5173\u7814\u7a76","text":"<p>Transformer \u57fa\u672c\u67b6\u6784\u4e0e\u539f\u7406\uff0c\u540e\u7eed\u6539\u8fdb\uff0c\u5305\u62ec\u4f4d\u7f6e\u7f16\u7801\u3001\u6ce8\u610f\u529b\u673a\u5236\u7b49\u7b49 ...</p> <ul> <li>Transformer: Attention Is All You Need. Ashish Vaswani et al. 2017. arxiv pdf<ul> <li>materials: video blog code</li> <li>\u6269\u5c55\u9605\u8bfb<ul> <li>\u8ba9\u7814\u7a76\u4eba\u5458\u7ede\u5c3d\u8111\u6c41\u7684Transformer\u4f4d\u7f6e\u7f16\u7801 - \u79d1\u5b66\u7a7a\u95f4|Scientific Spaces</li> </ul> </li> </ul> </li> </ul>"},{"location":"blog/2024/12/26/transformer-%E6%9E%B6%E6%9E%84%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6/#vision-transformer","title":"Vision Transformer","text":"<ul> <li> <p>ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at  Scale. Alexey Dosovitskiy et al. 2020. arxiv pdf video</p> </li> <li> <p>Swin-Transformer: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. Ze Liu et al. 2021. arxiv pdf video</p> </li> </ul>"},{"location":"blog/2024/12/26/transformer-%E6%9E%B6%E6%9E%84%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6/#multi-modality","title":"Multi-Modality","text":"<ul> <li>CLIP: Learning Transferable Visual Models From Natural Language Supervision. Alec Radford et al. 2021. arxiv pdf<ul> <li>CLIP: https://b23.tv/0rTiChh</li> </ul> </li> </ul>"},{"location":"blog/2024/12/26/transformer-%E6%9E%B6%E6%9E%84%E7%9B%B8%E5%85%B3%E7%A0%94%E7%A9%B6/#position-encoding","title":"Position Encoding","text":"<ul> <li>RoPE: {{https://arxiv.org/abs/2104.09864}}</li> </ul>"},{"location":"blog/archive/12/26/2024/","title":"12/26/2024","text":""},{"location":"blog/category/%E5%B7%A5%E5%85%B7%E6%89%8B%E5%86%8C/","title":"\u5de5\u5177\u624b\u518c","text":""},{"location":"blog/category/%E7%AC%94%E8%AE%B0%E6%95%99%E7%A8%8B/","title":"\u7b14\u8bb0&amp;\u6559\u7a0b","text":""},{"location":"blog/category/%E6%96%87%E7%8C%AE%E6%95%B4%E7%90%86/","title":"\u6587\u732e\u6574\u7406","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"blog/page/3/","title":"Blog","text":""},{"location":"blog/archive/12/26/2024/page/2/","title":"12/26/2024","text":""},{"location":"blog/archive/12/26/2024/page/3/","title":"12/26/2024","text":""},{"location":"blog/category/%E6%96%87%E7%8C%AE%E6%95%B4%E7%90%86/page/2/","title":"\u6587\u732e\u6574\u7406","text":""}]}