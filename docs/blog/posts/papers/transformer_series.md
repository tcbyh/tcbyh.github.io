---
date:
  created: 2024-12-26
categories:
  - 文献整理
---

# Transformer 架构相关研究

Transformer 基本架构与原理，后续改进，包括位置编码、注意力机制等等 ...

<!-- more -->

- Transformer: **Attention Is All You Need**. Ashish Vaswani et al. 2017. [arxiv](https://arxiv.org/abs/1706.03762) [pdf](pdfs/Transformer_Ashish_Vaswani_et_al_1706.03762.pdf)
    - materials: [video](https://b23.tv/IHQ3NBE) [blog](https://blog.csdn.net/qq_36667170/article/details/124359818) [code](https://github.com/jadore801120/attention-is-all-you-need-pytorch?tab=readme-ov-file)
    - 扩展阅读
        - [让研究人员绞尽脑汁的Transformer位置编码 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/8130?sharesource=weibo)

## Vision Transformer

- ViT: **An Image is Worth 16x16 Words: Transformers for Image Recognition at  Scale**. Alexey Dosovitskiy et al. 2020. [arxiv](https://arxiv.org/abs/2010.11929) [pdf](pdfs/ViT_Alexey_Dosovitskiy_et_al_2010.11929.pdf) [video](https://b23.tv/W4UdTX6)

- Swin-Transformer: **Swin Transformer: Hierarchical Vision Transformer using Shifted Windows**. Ze Liu et al. 2021. [arxiv](https://arxiv.org/abs/2103.14030) [pdf](pdfs/Swin-Transformer_Ze_Liu_et_al_2103.14030.pdf) [video](https://b23.tv/buruBr8)


## Multi-Modality

- CLIP: **Learning Transferable Visual Models From Natural Language Supervision**. Alec Radford et al. 2021. [arxiv](https://arxiv.org/abs/2103.00020) [pdf](pdfs/CLIP_Alec_Radford_et_al_2103.00020.pdf)
    - CLIP: https://b23.tv/0rTiChh


## Position Encoding

- RoPE: {{https://arxiv.org/abs/2104.09864}}
